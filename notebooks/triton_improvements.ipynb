{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d919f269-2d11-4d26-9f60-6ba9a56f32c3",
   "metadata": {},
   "source": [
    "# RWKV 6.X to Triton Port\n",
    "\n",
    "This notebook is designeed to help while porting RWKV6+ to triton. It first validates the algorithm in torch before porting to triton. Testbed for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "ef9bb6d1-6db4-4188-b477-76df527fbfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e5606-84d7-4857-bbee-bb7bfaf15363",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "id": "f5be6d20-c942-4ed7-8471-3bbc238157ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_inputs(B, H, L, K, V): \n",
    "    th.manual_seed(17)\n",
    "    device = \"cpu\"\n",
    "    rt = th.randn(B, H, L, K, device=device, requires_grad=True)\n",
    "    kt = th.randn(B, H, L, K, device=device, requires_grad=True)\n",
    "    vt = th.randn(B, H, L, V, device=device, requires_grad=True)\n",
    "    wt = -2 + 1e-1 * th.randn(B, H, L, K, V, device=device)\n",
    "    wt = -th.exp(wt)\n",
    "    wt.requires_grad = True\n",
    "    ut = th.randn(H, K, device=device, requires_grad=True)\n",
    "    return rt, kt, vt, wt, ut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df5bf84-55b6-446f-b02a-d6a7166e194b",
   "metadata": {},
   "source": [
    "## Diagnosis of 18/04/2024\n",
    "* [ ] [Songlin's code](https://github.com/sustcsonglin/flash-linear-attention/commit/fee90b2e72366a46c60e3ef16431133aa5aced8d) is wrong for the backward pass of `W`. Although it is hidden for normally-distributed values and high number of `V`. \n",
    "* [ ] TODO: extend U and W to support matrices. Experiments demonstrate it works wonders. Especially later in training.\n",
    "    * [This paper: The illusion of State in State Space Models](https://arxiv.org/pdf/2404.08819.pdf) demonstrates that a `matrix_cumprod` would give superior modelling ability. The matrix has to be non-diagonal as otherwise it would be a `cum_matmul` which would still be in `TC0` and not `NC1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919e1bd-12f2-4121-b10a-e1768cb311b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### rwkv_inner (from gptcore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8ae7e0be-b740-4d83-b371-4daeda7b6067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def rwkv_inner(r,k,v,w,u,kv_state,chunk_len:int=24,precision_dtype:th.dtype=th.float32):\n",
    "    \"\"\"\n",
    "    expects\n",
    "    r : (B,H,L,K)\n",
    "    k : (B,H,L,K)\n",
    "    v : (B,H,L,V)\n",
    "    w : (B,H,L,K) or (1,H,L,K)\n",
    "    u : (1,H,1,K)\n",
    "    kv_state : (B,H,K,V)\n",
    "    \"\"\"\n",
    "    B,H,L,K = k.size()\n",
    "    V = v.size(-1)\n",
    "    T = chunk_len\n",
    "\n",
    "    if L == 1:\n",
    "        kv = k.mT @ v\n",
    "        out = r @ (kv_state + u.mT * kv)\n",
    "        kv_state = w.mT * kv_state + kv\n",
    "        return out, kv_state\n",
    "    else:\n",
    "        # FIXME - support fast path for non-exact multiples\n",
    "        # ensure it's an exact multiple\n",
    "        if L % T != 0:\n",
    "            T = 1\n",
    "\n",
    "        N = L // T\n",
    "\n",
    "        # this has to be done to avoid numerical instability (inf/NaN) when w is used as a divisor up to chunk_length//2 places away (so precision_min_val^(T//2) has to be in fp range)\n",
    "        # NOTE - this does not account for the impact of the size of R, K so we currently use the chunk_len=32 numbers for chunk_len=24\n",
    "        assert(precision_dtype == th.float32 or precision_dtype == th.float64)\n",
    "        if precision_dtype == th.float32:\n",
    "            precision_min_val = 0.005 # good for fp32 (1.175e-38 ^ (1/16.0) < 0.00426)\n",
    "        else: #elif precision_dtype == torch.float64:\n",
    "            precision_min_val = 1e-10 # good for fp64 (1.7e-308 ^ (1/16.0) < 5.8e-20)\n",
    "        w = w.clamp(precision_min_val)\n",
    "\n",
    "        # calculate cumulative decay in log space where it won't overflow\n",
    "        w_log = w.float().log() # (1,H,L,K) or (B,H,L,K)\n",
    "\n",
    "        # chunked view of w_log\n",
    "        wc_log = w_log.view(w.size(0),H,N,T,K)\n",
    "        wc_log_cum = wc_log.cumsum(dim=-2)\n",
    "\n",
    "        # chunked view of shifted_w_log\n",
    "        shifted_wc_log_cum = F.pad(wc_log_cum, (0, 0, 1, -1))\n",
    "\n",
    "\n",
    "        # NOTE - we have to apply the decay weight from TWO ahead.. ONE ahead gets no decay (log==0)\n",
    "        # pre-applied weights\n",
    "        # left side is prior chunk (w_inter), right side is current chunk (w_intra)\n",
    "        # without u...\n",
    "        # w0   w1   w2   w3   | w4   w5   w6   w7          \n",
    "        # w1:4 w2:4 w3:4 w4:4 | w4:5 w4:6 w4:7 w4:8\n",
    "        # with u...\n",
    "        # w0   w1   w2   w3   | w4   w5   w6   w7          \n",
    "        # w1:4 w2:4 w3:4 w4:4 | w4:4 w4:5 w4:6 w4:7\n",
    "\n",
    "        # ws decays the entire current state (representing t-1) to the prior block (t-2)\n",
    "        ws = wc_log.sum(dim=-2, keepdim=True) # 1HN1K or BHN1K\n",
    "        # w_inter is the decay to the end of the current block, since it will be applied at the next iteration when current (t) becomes prior (t-1)\n",
    "        # this formula because e.g. w1:4 = w0:4 - w0:1\n",
    "        w_inter = ws - wc_log_cum # 1HNTK or BHNTK (w^(T-1) ... w^0)\n",
    "        # w_intra is the decay from the beginning of the current block (t), since it will be applied to current queries (t) against prior state (representing keys+values up to but not including block t)\n",
    "        # this formula because e.g. w1:3 = w0:3 - w0\n",
    "        w_intra = wc_log_cum - wc_log # 1HNTK or BHNTK (w^0 ... w^(T-2))\n",
    "\n",
    "        ws = list(ws.mT.exp().to(r.dtype).unbind(dim=-3)) # N x 1HK1 or BHK1 !!NOTE THE .mT HERE!!\n",
    "        w_inter = w_inter.exp().to(r.dtype) # 1HNTK or BHNTK\n",
    "        w_intra = w_intra.exp().to(r.dtype) # 1HNTK or BHNTK\n",
    "\n",
    "        # chunked view of r, k, v\n",
    "        r = r.view(B,H,N,T,K) \n",
    "        k = k.view(B,H,N,T,K) \n",
    "        v = v.view(B,H,N,T,V)\n",
    "        u = u.unsqueeze(2).to(r.dtype) # (1,H,1,1,K)\n",
    "\n",
    "        # parallel calculation of all intra-chunk attention contributions\n",
    "        wc_log_offset = shifted_wc_log_cum[...,T//2:T//2+1,:] # B,H,N,1,K\n",
    "        r_decay = (shifted_wc_log_cum - wc_log_offset).to(precision_dtype).exp() # B,H,N,T,K\n",
    "        k_inv_decay = (wc_log_offset - wc_log_cum).to(precision_dtype).exp() # B,H,N,T,K\n",
    "        a = ((r*r_decay) @ (k*k_inv_decay).mT).to(r.dtype).tril(-1) # B,H,N,T,T\n",
    "        # add u term to attention (NOTE - the tril(-1) above zeroed the diagonal)\n",
    "        a = a + th.einsum('bhntk,bhntk->bhnt', r, u * k).diag_embed()\n",
    "        out = a @ v # BHNTV\n",
    "        # alternate way of adding in u\n",
    "        # out = out + torch.einsum('bhntk,bhntk,bhntv->bhntv', r, u * k, v) \n",
    "\n",
    "        # parallel precalculation of chunked (k*wk).mT@v for use in recurrent state calc below\n",
    "        wkv = (k * w_inter).mT @ v # BHNKV\n",
    "        wkv = list(wkv.unbind(dim=-3)) # N x BHKV\n",
    "\n",
    "        # recurrent calculation of all states\n",
    "        states = []\n",
    "        for i in range(N):\n",
    "            states.append(kv_state)\n",
    "            kv_state = kv_state * ws[i] + wkv[i] # BHKV\n",
    "            # equivalent non-precalced version\n",
    "            #wkv = (k[...,i,:,:] * wk[...,i,:,:]).mT @ v[...,i,:,:]\n",
    "            #kv_state = kv_state * ws[i] + wkv\n",
    "        states = th.stack(states, dim=2) # BHNKV       \n",
    "\n",
    "        # parallel application of all r to states\n",
    "        out = out + (r * w_intra) @ states # BHNTV\n",
    "        out = out.view(B,H,L,V)\n",
    "        return out, kv_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e66818-cecf-49c9-b134-8e25d9a6f6ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Base Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "c5f533eb-caaa-45be-8568-9be6be86fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "def naive_recurrent_rwkv6_original(q, k, v, w, u, initial_state=None, output_final_state=False):\n",
    "    orig_dtype = q.dtype\n",
    "    q, k, v, w, u = map(lambda x: x.float(), (q, k, v, w, u))\n",
    "    batch_size, n_heads, seq_len, d_head_k = q.shape\n",
    "    _, _, _, d_head_v = v.shape\n",
    "    h = torch.zeros(batch_size, n_heads, d_head_k, d_head_v, dtype=torch.float32, device=q.device)\n",
    "    o = torch.zeros_like(v)\n",
    "\n",
    "    if initial_state is not None:\n",
    "        h += initial_state\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        q_i = q[:, :, i, :]\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i, :]\n",
    "        w_i = w[:, :, i].exp()\n",
    "        kv_i = k_i[..., None] * v_i[..., None, :]\n",
    "        o_i = (h + u[None, ..., None] * kv_i) * q_i[..., None]\n",
    "        o[:, :, i] = o_i.sum(-2)\n",
    "        h = h * w_i[..., None] + kv_i\n",
    "    return o.to(orig_dtype)\n",
    "\n",
    "\n",
    "def naive_recurrent_rwkv6_bwd_original(q, k, v, w, u, o, do, initial_state=None, output_final_state=False):\n",
    "    q, k, v, w, u, o, do = map(lambda x: x.float(), (q, k, v, w, u, o, do))\n",
    "    batch_size, n_heads, seq_len, d_head_k = q.shape\n",
    "    _, _, _, d_head_v = v.shape\n",
    "    h = torch.zeros(batch_size, n_heads, d_head_k, d_head_v, dtype=torch.float32, device=q.device)\n",
    "    dq = torch.zeros_like(q)\n",
    "    dq_aux = torch.zeros_like(q)\n",
    "\n",
    "    if initial_state is not None:\n",
    "        h += initial_state\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i]\n",
    "        w_i = w[:, :, i].exp()\n",
    "        kv_i = k_i[..., None] * v_i[..., None, :]\n",
    "        h_i = (h + u[None, ..., None] * kv_i)\n",
    "        dq_i = (do[:, :, i, None, :] * h_i).sum(-1)\n",
    "        dq_aux_i = (do[:, :, i, None, :] * h).sum(-1)\n",
    "        dq[:, :, i] = dq_i\n",
    "        dq_aux[:, :, i] = dq_aux_i\n",
    "        h = h * w_i[..., None] + kv_i\n",
    "\n",
    "    du = u.new_zeros(batch_size, n_heads, d_head_k)\n",
    "    dh = torch.zeros_like(h)\n",
    "    dk = torch.zeros_like(k)\n",
    "    dk_aux = torch.zeros_like(k)\n",
    "    dv = torch.zeros_like(v)\n",
    "\n",
    "    for i in range(seq_len-1, -1, -1):\n",
    "        d_kv_i = do[:, :, i, None, :] * q[:, :, i, :, None]\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i]\n",
    "        du_i = (d_kv_i * k_i[..., None] * v_i[..., None, :]).sum(-1)\n",
    "        du += du_i\n",
    "        dk_i = (dh * v_i[..., None, :]).sum(-1)\n",
    "        dk_aux[:, :, i] = dk_i\n",
    "        dk_i += (d_kv_i * u[None, ..., None] * v_i[..., None, :]).sum(-1)\n",
    "        dv_i = (d_kv_i * u[None, ..., None] * k_i[..., None]).sum(-2)\n",
    "        dv_i += (dh * k_i[..., None]).sum(-2)\n",
    "\n",
    "        dk[:, :, i] = dk_i\n",
    "        dv[:, :, i] = dv_i\n",
    "        dh = dh * w[:, :, i, :, None].exp() + d_kv_i\n",
    "\n",
    "    # dw = q * dq_aux - k * dk_aux\n",
    "    dw = torch.zeros_like(w)\n",
    "    for i in range(seq_len-2, 0, -1):\n",
    "        dw[:, :, i] = dw[:, :, i+1] + dq_aux[:, :, i+1] * q[:, :, i+1] - dk_aux[:, :, i] * k[:, :, i]\n",
    "\n",
    "    du = du.sum(0)\n",
    "    return dq, dk, dv, dw, du"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e42b78-67bb-4d70-bca8-388dd539a070",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Demonstrate Songlin's Code is Wrong: \n",
    "Songlin's code is wrong! Hidden by a high `|V|`\n",
    "\n",
    "* `V = 1` and errors era ~1e-2\n",
    "* `V = 64` and errors era ~1e-9\n",
    "\n",
    "`Autograd(recurrent_naive_fw)` and `rwkv_inner` match (0 error) but both disagree with `recurrent_naive_bw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "a7298ecc-340f-451e-8d53-67b7c6148b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00,  1.0814e-04, -9.6143e-04,  4.2107e-03, -2.9855e-02,\n",
       "        -5.6489e-02, -5.2342e-02,  6.2052e-02,  5.8009e-02,  5.6197e-02,\n",
       "        -3.9192e-02,  2.2172e-01,  4.5362e-01,  2.9285e-01,  1.9845e-01,\n",
       "         0.0000e+00], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 833,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementation is wrong. hidden by high V (ex. 64)\n",
    "B, H, L, K, V = 1, 1, 16, 1, 1\n",
    "\n",
    "#### AUTOGRAD FORWARD PASS\n",
    "rt, kt, vt, wt, ut = gen_inputs(B, H, L, K, V)\n",
    "o = naive_recurrent_rwkv6_original(rt, kt, vt, wt[..., 0], ut[..., 0])\n",
    "grads = []\n",
    "o.register_hook(lambda d:grads.append(d))\n",
    "o.mean().backward()\n",
    "do = grads[0]\n",
    "\n",
    "#### Autograd rwkv_inner\n",
    "rt3, kt3, vt3, wt3, ut3 = gen_inputs(B, H, L, K, V)\n",
    "o3, state3 = rwkv_inner(rt3, kt3, vt3, wt3[..., 0], ut3.view(1, H, 1, K), th.zeros(B, H, K, V))\n",
    "o3.mean().backward()\n",
    "\n",
    "#### MANUAL BACKWARD PASS\n",
    "rt2, kt2, vt2, wt2, ut2 = gen_inputs(B, H, L, K, V)\n",
    "dq, dk, dv, dw, du = naive_recurrent_rwkv6_bwd_original(rt2, kt2, vt2, wt2[..., 0], ut2[..., 0], o, do)\n",
    "dw = dw[..., None]\n",
    "\n",
    "# (wt.grad - dw).flatten(), (wt.grad - wt3.grad).flatten()\n",
    "(dw - wt3.grad).flatten() # (rt.grad - dq).flatten() \n",
    "\n",
    "#### DOESN'T MATCH!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "63430906-5a27-4f45-a755-721badae858e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2.9104e-09,  2.9104e-09, -6.7521e-09, -6.7521e-09, -4.7104e-07,\n",
       "         -1.5335e-07, -2.3590e-04, -8.0526e-05,  5.6382e-04,  2.2494e-04,\n",
       "          6.9353e-04,  2.4993e-04, -1.8924e-03, -6.7232e-04,  5.9796e-04,\n",
       "          2.2419e-04, -2.4683e-03, -7.2213e-04,  1.1753e-02,  3.9835e-03,\n",
       "         -8.7115e-04, -2.8660e-04,  3.4635e-03,  1.1052e-03,  6.2721e-04,\n",
       "          2.2001e-04,  2.4188e-03,  8.6338e-04, -4.9119e-04, -1.4935e-04,\n",
       "          7.1844e-04,  2.2646e-04, -1.1038e-02, -3.6537e-03,  1.0063e-03,\n",
       "          3.1321e-04, -1.1371e-03, -3.4617e-04, -4.1771e-04, -1.4600e-04,\n",
       "         -1.7502e-03, -5.3780e-04, -2.2888e-04, -7.8782e-05, -8.0950e-05,\n",
       "         -2.5472e-05, -1.9536e-03, -6.3890e-04,  7.7323e-03,  2.8001e-03,\n",
       "         -4.6986e-02, -1.6366e-02, -4.9570e-04, -1.6514e-04,  3.0733e-03,\n",
       "          9.5976e-04, -7.9752e-04, -2.5922e-04, -3.2619e-04, -1.0276e-04,\n",
       "          1.8081e-03,  5.8611e-04,  3.0957e-04,  8.5090e-05,  1.8494e-03,\n",
       "          6.8724e-04,  1.7429e-03,  5.5131e-04,  1.4735e-02,  5.0678e-03,\n",
       "         -1.9612e-03, -6.3278e-04,  5.5735e-03,  1.7489e-03,  5.8457e-04,\n",
       "          1.9825e-04,  1.4576e-04,  4.2671e-05, -1.4422e-03, -4.7465e-04,\n",
       "          4.7826e-03,  1.4802e-03, -1.2426e-03, -4.1503e-04,  5.6292e-03,\n",
       "          1.9057e-03, -3.1180e-03, -1.0188e-03,  2.4422e-03,  7.1864e-04,\n",
       "         -1.7973e-02, -6.2780e-03, -2.6255e-03, -8.3161e-04, -3.0739e-03,\n",
       "         -8.9006e-04, -6.4643e-03, -1.9320e-03, -7.2814e-04, -2.1262e-04,\n",
       "          3.1104e-03,  1.0577e-03,  6.1993e-04,  2.2681e-04,  6.2623e-05,\n",
       "          1.8783e-05, -9.0354e-05, -2.7568e-05,  9.3171e-04,  2.9006e-04,\n",
       "          3.3178e-05,  1.1094e-05,  6.2143e-03,  2.0485e-03,  4.3108e-03,\n",
       "          1.3173e-03,  3.0125e-03,  8.8216e-04, -1.9723e-03, -6.1440e-04,\n",
       "          8.3851e-04,  2.9802e-04, -4.0276e-02, -1.4846e-02, -0.0000e+00,\n",
       "         -0.0000e+00, -0.0000e+00, -0.0000e+00]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([ 2.9104e-09,  2.9104e-09, -6.7521e-09, -6.7521e-09, -1.5335e-07,\n",
       "         -1.5335e-07, -8.0526e-05, -8.0526e-05,  2.2494e-04,  2.2494e-04,\n",
       "          2.4993e-04,  2.4993e-04, -6.7232e-04, -6.7232e-04,  2.2419e-04,\n",
       "          2.2419e-04, -7.2213e-04, -7.2213e-04,  3.9835e-03,  3.9835e-03,\n",
       "         -2.8660e-04, -2.8660e-04,  1.1052e-03,  1.1052e-03,  2.2001e-04,\n",
       "          2.2001e-04,  8.6338e-04,  8.6338e-04, -1.4935e-04, -1.4935e-04,\n",
       "          2.2646e-04,  2.2646e-04, -3.6537e-03, -3.6537e-03,  3.1321e-04,\n",
       "          3.1321e-04, -3.4617e-04, -3.4617e-04, -1.4600e-04, -1.4600e-04,\n",
       "         -5.3780e-04, -5.3780e-04, -7.8782e-05, -7.8782e-05, -2.5472e-05,\n",
       "         -2.5472e-05, -6.3890e-04, -6.3890e-04,  2.8001e-03,  2.8001e-03,\n",
       "         -1.6366e-02, -1.6366e-02, -1.6514e-04, -1.6514e-04,  9.5976e-04,\n",
       "          9.5976e-04, -2.5922e-04, -2.5922e-04, -1.0276e-04, -1.0276e-04,\n",
       "          5.8611e-04,  5.8611e-04,  8.5090e-05,  8.5090e-05,  6.8724e-04,\n",
       "          6.8724e-04,  5.5131e-04,  5.5131e-04,  5.0678e-03,  5.0678e-03,\n",
       "         -6.3278e-04, -6.3278e-04,  1.7489e-03,  1.7489e-03,  1.9825e-04,\n",
       "          1.9825e-04,  4.2671e-05,  4.2671e-05, -4.7465e-04, -4.7465e-04,\n",
       "          1.4802e-03,  1.4802e-03, -4.1503e-04, -4.1503e-04,  1.9057e-03,\n",
       "          1.9057e-03, -1.0188e-03, -1.0188e-03,  7.1864e-04,  7.1864e-04,\n",
       "         -6.2780e-03, -6.2780e-03, -8.3161e-04, -8.3161e-04, -8.9006e-04,\n",
       "         -8.9006e-04, -1.9320e-03, -1.9320e-03, -2.1262e-04, -2.1262e-04,\n",
       "          1.0577e-03,  1.0577e-03,  2.2681e-04,  2.2681e-04,  1.8783e-05,\n",
       "          1.8783e-05, -2.7568e-05, -2.7568e-05,  2.9006e-04,  2.9006e-04,\n",
       "          1.1094e-05,  1.1094e-05,  2.0485e-03,  2.0485e-03,  1.3173e-03,\n",
       "          1.3173e-03,  8.8216e-04,  8.8216e-04, -6.1440e-04, -6.1440e-04,\n",
       "          2.9802e-04,  2.9802e-04, -1.4846e-02, -1.4846e-02, -0.0000e+00,\n",
       "         -0.0000e+00, -0.0000e+00, -0.0000e+00]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementation is wrong. hidden by high V (ex. 64)\n",
    "B, H, L, K, V = 1, 1, 32, 2, 2\n",
    "\n",
    "#### AUTOGRAD FORWARD PASS\n",
    "rt, kt, vt, wt, ut = gen_inputs(B, H, L, K, V)\n",
    "w_ = -th.exp(wt)\n",
    "o = naive_recurrent_rwkv6_original(rt, kt, vt, w_[..., 0], ut[..., 0])\n",
    "grads = []\n",
    "o.register_hook(lambda d:grads.append(d))\n",
    "o.mean().backward()\n",
    "do = grads[0]\n",
    "\n",
    "#### Autograd rwkv_inner\n",
    "rt3, kt3, vt3, wt3, ut3 = gen_inputs(B, H, L, K, V)\n",
    "w3_ = -th.exp(wt3)\n",
    "o3, state3 = rwkv_inner(rt3, kt3, vt3, w3_[..., 0], ut3.view(1, H, 1, K), th.zeros(B, H, K, V))\n",
    "o3.mean().backward()\n",
    "\n",
    "#### MANUAL BACKWARD PASS\n",
    "rt2, kt2, vt2, wt2, ut2 = gen_inputs(B, H, L, K, V)\n",
    "w2_ = -th.exp(wt2)\n",
    "dq, dk, dv, dw, du = naive_recurrent_rwkv6_bwd_original(rt2, kt2, vt2, w2_[..., 0], ut2[..., 0], o, do)\n",
    "dw = dw[..., None]\n",
    "(wt.grad - dw).detach().flatten(), (w_ - w2_).detach().flatten(), (wt3.grad - dw).detach().flatten(), (w3_ - w2_).detach().flatten()\n",
    "\n",
    "#### DOESN'T MATCH!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6df76d4f-91f4-43c8-be24-4cf01163ef46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 16, 1, 64]), torch.Size([1, 1, 16, 1, 1]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt.grad.shape, dw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a8b7c2-bc11-4eb3-b23c-1011ed26bd6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Manual: Fixing backward implementationÂ¶ - DISCARDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0160f72f-d91a-4c3a-86b7-32c1f86232be",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4\n",
    "q_ic, k_jc, v_jd, w_tc_prelog, _ = gen_inputs(1, 1, L, 1, 1)\n",
    "w_tc = -th.exp(w_tc_prelog)\n",
    "\n",
    "q_ic, k_jc, v_jd, w_tc = map(lambda x: x[0, 0].detach.requires_grad(True), (q_ic, k_jc, v_jd, w_tc))\n",
    "\n",
    "wcum_jc_buffer = []\n",
    "for j in range(1, L+1): \n",
    "    wcum_jc_buffer.append( w_tc[:j].cumsum(dim=0).exp() )\n",
    "wcum_jc = th.stack(wcum_jc_buffer)\n",
    "        \n",
    "inner_jcd = th.einsum('jc,jc,jd->jcd', wcum_jc, k_jc, v_jd)\n",
    "wkv_icd = th.cumsum(inner_jcd, dim=0)\n",
    "oid = th.einsum('ic,icd->id', q_ic, wkv_icd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec8c5b-61b3-48ac-815b-002fcffc58c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a96da10-9392-472d-9cee-126cade7eaf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30da04c7-19f3-4626-afd0-627a77241484",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Fixing backward implementation - algorithmically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "102d8194-1eed-482f-823d-4b12a87488ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_recurrent_rwkv6_bwd_hypnofix_correct(q, k, v, w, u, o, do, initial_state=None, output_final_state=False):\n",
    "    q, k, v, w, u, o, do = map(lambda x: x.float(), (q, k, v, w, u, o, do))\n",
    "    batch_size, n_heads, seq_len, d_head_k = q.shape\n",
    "    _, _, _, d_head_v = v.shape\n",
    "    h = torch.zeros(batch_size, n_heads, d_head_k, d_head_v, dtype=torch.float32, device=q.device)\n",
    "    dq = torch.zeros_like(q)\n",
    "    dq_aux = torch.zeros_like(q)\n",
    "\n",
    "    if initial_state is not None:\n",
    "        h += initial_state\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i]\n",
    "        w_i = w[:, :, i].exp()\n",
    "        do_i = do[:, :, i]\n",
    "        kv_i = k_i[..., None] * v_i[..., None, :]\n",
    "        dq[:, :, i] = ((h * + kv_i * u[None, ..., None]) * do_i[None, :]).sum(dim=-1)\n",
    "        dq_aux[:, :, i] = (h * do_i[None, :]).sum(-1)\n",
    "        h = h * w_i[..., None] + kv_i\n",
    "        \n",
    "    du = u.new_zeros(batch_size, n_heads, d_head_k)\n",
    "    dh = torch.zeros_like(h)\n",
    "    dk = torch.zeros_like(k)\n",
    "    dw = torch.zeros_like(w)\n",
    "    dv = torch.zeros_like(v)\n",
    "\n",
    "    for i in range(seq_len-1, -1, -1):\n",
    "        d_kv_i = do[:, :, i, None, :] * q[:, :, i, :, None]\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i]\n",
    "        kvi = k_i[..., None] * v_i[..., None, :]\n",
    "        du_i = (d_kv_i * kvi).sum(-1)\n",
    "        du += du_i\n",
    "        \n",
    "        d_kv_hiu = (dh + d_kv_i * u[None, ..., None])\n",
    "        dk[:, :, i] = (d_kv_hiu * v_i[..., None, :]).sum(-1)\n",
    "        dv[:, :, i] = (d_kv_hiu * k_i[..., :, None]).sum(-2)\n",
    "        \n",
    "        # backward on pure W\n",
    "        # back_w += w[:, :, i]\n",
    "        # dw[:, :, i] = dw[:, :, i+1] + (q[:, :, i] * back_w * kvi).sum(-1)\n",
    "\n",
    "        # since U, W vector, collapse si into C\n",
    "        if seq_len - i - 1: \n",
    "            wcum = th.zeros_like(w[:,:, i+1:])\n",
    "            for t in range(seq_len - i - 1): \n",
    "                wcum[:, :, t] = w[:, :, i:-1].sum(dim=2)\n",
    "            # c,c,d -> cd -> c\n",
    "            si = (do[:, :, [i], None, :] * q[:, :, i+1:, ..., None] * k[:, :, i+1:, ..., None] * v[:, :, i+1:, ..., None, :]).sum(-1)\n",
    "            dw[:, :, i] = (wcum.exp() * si).sum(2)\n",
    "            \n",
    "        # dk_aux[:, :, i] = (dh * v_i[..., None, :] * k_i[..., None]).sum(-1)\n",
    "        dh = dh * w[:, :, i, :, None].exp() + d_kv_i\n",
    "    \n",
    "    du = du.sum(0)\n",
    "    return dq, dk, dv, dw, du"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed172b-b322-46b7-84f0-53857edb3681",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Fixing backward implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "93c58005-3178-4255-9175-a9d01c1d9bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_recurrent_rwkv6_bwd_hypnofix(q, k, v, w, u, o, do, initial_state=None, output_final_state=False):\n",
    "    q, k, v, w, u, o, do = map(lambda x: x.float(), (q, k, v, w, u, o, do))\n",
    "    batch_size, n_heads, seq_len, d_head_k = q.shape\n",
    "    _, _, _, d_head_v = v.shape\n",
    "    h = torch.zeros(batch_size, n_heads, d_head_k, d_head_v, dtype=torch.float32, device=q.device)\n",
    "    dq = torch.zeros_like(q)\n",
    "    dq_aux = torch.zeros_like(q)\n",
    "\n",
    "    if initial_state is not None:\n",
    "        h += initial_state\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i]\n",
    "        w_i = w[:, :, i].exp()\n",
    "        do_i = do[:, :, i]\n",
    "        kv_i = k_i[..., None] * v_i[..., None, :]\n",
    "        dq[:, :, i] = ((h * + kv_i * u[None, ..., None]) * do_i[None, :]).sum(dim=-1)\n",
    "        dq_aux[:, :, i] = (h * do_i[None, :]).sum(-1)\n",
    "        h = h * w_i[..., None] + kv_i\n",
    "        \n",
    "    du = u.new_zeros(batch_size, n_heads, d_head_k)\n",
    "    dh = torch.zeros_like(h)\n",
    "    dk = torch.zeros_like(k)\n",
    "    dw = torch.zeros_like(w)\n",
    "    dv = torch.zeros_like(v)\n",
    "\n",
    "    dwh = u.new_ones(batch_size, n_heads, d_head_k)\n",
    "    \n",
    "    for i in range(seq_len-1, -1, -1):\n",
    "        d_kv_i = do[:, :, i, None, :] * q[:, :, i, :, None]\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i]\n",
    "        kvi = k_i[..., None] * v_i[..., None, :]\n",
    "        du_i = (d_kv_i * kvi).sum(-1)\n",
    "        du += du_i\n",
    "        \n",
    "        d_kv_hiu = (dh + d_kv_i * u[None, ..., None])\n",
    "        dk[:, :, i] = (d_kv_hiu * v_i[..., None, :]).sum(-1)\n",
    "        dv[:, :, i] = (d_kv_hiu * k_i[..., :, None]).sum(-2)\n",
    "        \n",
    "        # backward on pure W\n",
    "        # back_w += w[:, :, i]\n",
    "        # dw[:, :, i] = dw[:, :, i+1] + (q[:, :, i] * back_w * kvi).sum(-1)\n",
    "        \n",
    "        if i < seq_len-1: \n",
    "            wwh *= wiexp\n",
    "            # dwh = w[:, :, i:-1].sum(dim=2).exp()\n",
    "            dw[:, :, i] = dwh * si\n",
    "        \n",
    "        # si = (q[:, :, i, ..., None] * kvi).sum(-1)  \n",
    "        si = (do[:, :, i, None, :] * q[:, :, i, ..., None] * kvi).sum(-1)     \n",
    "        \n",
    "        # dk_aux[:, :, i] = (dh * v_i[..., None, :] * k_i[..., None]).sum(-1)\n",
    "        wiexp = w[:, :, i, :].exp()\n",
    "        dh = dh * wiexp[..., None] + d_kv_i\n",
    "    \n",
    "    du = du.sum(0)\n",
    "    return dq, dk, dv, dw, du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5a6ef4b0-2abd-4142-ae31-08eb94406d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.8016e-16,  8.8016e-16, -1.1330e-14, -1.1330e-14,  1.1148e-05,\n",
       "          9.4622e-15,  2.5693e-04, -1.0575e-13, -7.0574e-05,  5.8352e-14,\n",
       "          5.6913e-04, -7.8122e-13,  2.0061e-03,  6.5811e-17, -1.4693e-03,\n",
       "         -5.2914e-12, -1.5169e-03,  1.5346e-12, -9.8088e-04, -4.6428e-11,\n",
       "         -3.2685e-03, -1.9871e-11, -1.5882e-02, -2.9842e-10,  1.3238e-02,\n",
       "         -2.0407e-10, -1.4260e-02, -1.0307e-09, -2.0520e-03, -2.7139e-09,\n",
       "         -2.1849e-03,  9.5263e-10, -1.0940e-02, -1.8337e-08, -7.8895e-03,\n",
       "          1.1356e-08,  1.7036e-05,  1.0656e-07, -1.3284e-03, -3.9607e-07,\n",
       "          1.5412e-02,  6.4054e-07,  6.4981e-04, -2.3626e-06, -7.5737e-04,\n",
       "          1.1148e-06, -1.8389e-03, -1.8949e-05, -3.8423e-03,  1.1038e-05,\n",
       "          1.0049e-02, -9.4611e-06, -1.6737e-03,  4.2742e-04, -9.3307e-03,\n",
       "          9.3694e-04,  1.4482e-03,  6.9880e-04,  3.3894e-03,  2.1426e-03,\n",
       "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([ 8.8016e-16,  8.8016e-16, -1.1330e-14, -1.1330e-14,  9.4622e-15,\n",
       "          9.4622e-15, -1.0575e-13, -1.0575e-13,  5.8352e-14,  5.8352e-14,\n",
       "         -7.8122e-13, -7.8122e-13,  6.5811e-17,  6.5811e-17, -5.2914e-12,\n",
       "         -5.2914e-12,  1.5346e-12,  1.5346e-12, -4.6428e-11, -4.6428e-11,\n",
       "         -1.9871e-11, -1.9871e-11, -2.9842e-10, -2.9842e-10, -2.0407e-10,\n",
       "         -2.0407e-10, -1.0307e-09, -1.0307e-09, -2.7139e-09, -2.7139e-09,\n",
       "          9.5263e-10,  9.5263e-10, -1.8337e-08, -1.8337e-08,  1.1356e-08,\n",
       "          1.1356e-08,  1.0656e-07,  1.0656e-07, -3.9607e-07, -3.9607e-07,\n",
       "          6.4054e-07,  6.4054e-07, -2.3626e-06, -2.3626e-06,  1.1148e-06,\n",
       "          1.1148e-06, -1.8949e-05, -1.8949e-05,  1.1038e-05,  1.1038e-05,\n",
       "         -9.4611e-06, -9.4611e-06,  4.2742e-04,  4.2742e-04,  9.3694e-04,\n",
       "          9.3694e-04,  6.9880e-04,  6.9880e-04,  2.1426e-03,  2.1426e-03,\n",
       "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementation is wrong. hidden by high V (ex. 64)\n",
    "B, H, L, K, V = 1, 1, 16, 2, 2\n",
    "\n",
    "#### AUTOGRAD FORWARD PASS\n",
    "rt, kt, vt, wt, ut = gen_inputs(B, H, L, K, V)\n",
    "w_ = -th.exp(wt)\n",
    "o = naive_recurrent_rwkv6_original(rt, kt, vt, w_[..., 0], ut[..., 0])\n",
    "grads = []\n",
    "o.register_hook(lambda d:grads.append(d))\n",
    "o.mean().backward()\n",
    "do = grads[0]\n",
    "\n",
    "#### Autograd rwkv_inner\n",
    "rt3, kt3, vt3, wt3, ut3 = gen_inputs(B, H, L, K, V)\n",
    "w3_ = -th.exp(wt3)\n",
    "o3, state3 = rwkv_inner(rt3, kt3, vt3, w3_[..., 0], ut3.view(1, H, 1, K), th.zeros(B, H, K, V))\n",
    "o3.mean().backward()\n",
    "\n",
    "#### MANUAL BACKWARD PASS\n",
    "rt2, kt2, vt2, wt2, ut2 = gen_inputs(B, H, L, K, V)\n",
    "w2_ = -th.exp(wt2)\n",
    "dq, dk, dv, dw, du = naive_recurrent_rwkv6_bwd_hypnofix_correct(rt2, kt2, vt2, w2_[..., 0], ut2[..., 0], o, do)\n",
    "dw = dw[..., None]\n",
    "(wt.grad - dw).detach().flatten(), (w_ - w2_).detach().flatten(), (wt3.grad - dw).detach().flatten(), (w3_ - w2_).detach().flatten()\n",
    "\n",
    "#### DOESN'T MATCH!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b5e4b-ba22-4144-b80a-25752e0bb717",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### My funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da08269a-4af5-4474-a702-9e972fbf7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_recurrent_rwkv6(q, k, v, w, u, initial_state=None, output_final_state=False):\n",
    "    orig_dtype = q.dtype\n",
    "    q, k, v, w, u = map(lambda x: x.float(), (q, k, v, w, u))\n",
    "    batch_size, n_heads, seq_len, d_head_k = q.shape\n",
    "    _, _, _, d_head_v = v.shape\n",
    "    h = torch.zeros(batch_size, n_heads, d_head_k, d_head_v, dtype=torch.float32, device=q.device)\n",
    "    o = torch.zeros_like(v)\n",
    "\n",
    "    if initial_state is not None:\n",
    "        h += initial_state\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        q_i = q[:, :, i, :]\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i, :]\n",
    "        w_i = w[:, :, i].exp()\n",
    "        kv_i = k_i[..., None] * v_i[..., None, :]\n",
    "        # print(f\"h.shape: {h.shape}, u.shape: {u[None, ..., None].shape} and kv.shape: {kv_i.shape}\")\n",
    "        h_i = (h + u[None, ..., None] * kv_i)\n",
    "        o_i = th.einsum('bhc,bhcd->bhd', q_i, h_i)\n",
    "        o[:, :, i] = o_i.sum(-2)\n",
    "        h = h * w_i + kv_i\n",
    "    return o.to(orig_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aff4ca3-785a-4436-bb62-4279b25f2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_recurrent_rwkv6_bwd(q, k, v, w, u, o, do, initial_state=None, output_final_state=False):\n",
    "    q, k, v, w, u, o, do = map(lambda x: x.float(), (q, k, v, w, u, o, do))\n",
    "    batch_size, n_heads, seq_len, d_head_k = q.shape\n",
    "    _, _, _, d_head_v = v.shape\n",
    "    h = torch.zeros(batch_size, n_heads, d_head_k, d_head_v, dtype=torch.float32, device=q.device)\n",
    "    dq = torch.zeros_like(q)\n",
    "    dq_aux = torch.zeros_like(q)\n",
    "\n",
    "    if initial_state is not None:\n",
    "        h += initial_state\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i]\n",
    "        w_i = w[:, :, i].exp()\n",
    "        kv_i = k_i[..., None] * v_i[..., None, :]\n",
    "        h_i = (h + u[None, ..., None] * kv_i)\n",
    "        dq_i = (do[:, :, i, None, :] * h_i).sum(-1)\n",
    "        dq_aux_i = (do[:, :, i, None, :] * h).sum(-1)\n",
    "        dq[:, :, i] = dq_i\n",
    "        dq_aux[:, :, i] = dq_aux_i\n",
    "        h = h * w_i + kv_i\n",
    "\n",
    "    du = torch.zeros(batch_size, n_heads, d_head_k)\n",
    "    dh = torch.zeros_like(h)\n",
    "    dk = torch.zeros_like(k)\n",
    "    dk_aux = torch.zeros_like(k)\n",
    "    dv = torch.zeros_like(v)\n",
    "\n",
    "    for i in range(seq_len-1, -1, -1):\n",
    "        d_kv_i = do[:, :, i, None, :] * q[:, :, i, :, None]\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i]\n",
    "        du_i = (d_kv_i * k_i[..., None] * v_i[..., None, :]).sum(-1)\n",
    "        du += du_i\n",
    "        dk_i = (dh * v_i[..., None, :]).sum(-1)\n",
    "        dk_aux[:, :, i] = dk_i\n",
    "        dk_i += (d_kv_i * u[None, ..., None] * v_i[..., None, :]).sum(-1)\n",
    "        dv_i = (d_kv_i * u[None, ..., None] * k_i[..., None]).sum(-2)\n",
    "        dv_i += (dh * k_i[..., None]).sum(-2)\n",
    "\n",
    "        dk[:, :, i] = dk_i\n",
    "        dv[:, :, i] = dv_i\n",
    "        dh = dh * w[:, :, i, :, :].exp() + d_kv_i\n",
    "\n",
    "    # dw = q * dq_aux - k * dk_aux\n",
    "    dw = torch.zeros_like(w)\n",
    "    for i in range(seq_len-2, -1, -1):\n",
    "        dw[:, :, i] = dw[:, :, i+1] + (dq_aux[:, :, i+1] * q[:, :, i+1] - dk_aux[:, :, i] * k[:, :, i])[..., None]\n",
    "\n",
    "    du = du.sum(0)\n",
    "    return dq, dk, dv, dw, du"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6d0a2-450f-46dc-834e-362b2f4878d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### My tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "071dec21-b5b4-4fd1-be61-3735be79e1f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "gen_inputs() missing 5 required positional arguments: 'B', 'H', 'L', 'K', and 'V'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#### MANUAL BACKWARD PASS\u001b[39;00m\n\u001b[1;32m     13\u001b[0m do \u001b[38;5;241m=\u001b[39m grads[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m rt2, kt2, vt2, wt2, ut2 \u001b[38;5;241m=\u001b[39m \u001b[43mgen_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m w_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mth\u001b[38;5;241m.\u001b[39mexp(wt2)\n\u001b[1;32m     16\u001b[0m dq, dk, dv, dw, du \u001b[38;5;241m=\u001b[39m naive_recurrent_rwkv6_bwd(rt2, kt2, vt2, w_, ut2, o, do)\n",
      "\u001b[0;31mTypeError\u001b[0m: gen_inputs() missing 5 required positional arguments: 'B', 'H', 'L', 'K', and 'V'"
     ]
    }
   ],
   "source": [
    "B, H, L, K, V = 1, 1, 8, 1, 1\n",
    "\n",
    "#### AUTOGRAD FORWARD PASS\n",
    "rt, kt, vt, wt, ut = gen_inputs(B, H, L, K, V)\n",
    "w_ = -th.exp(wt)\n",
    "o = naive_recurrent_rwkv6(rt, kt, vt, w_, ut)\n",
    "# get grad of nonleaf variable\n",
    "grads = []\n",
    "o.register_hook(lambda d:grads.append(d))\n",
    "o.mean().backward()\n",
    "\n",
    "#### MANUAL BACKWARD PASS\n",
    "do = grads[0]\n",
    "rt2, kt2, vt2, wt2, ut2 = gen_inputs()\n",
    "w_ = -th.exp(wt2)\n",
    "dq, dk, dv, dw, du = naive_recurrent_rwkv6_bwd(rt2, kt2, vt2, w_, ut2, o, do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b24464-9b4f-48e6-a5e5-1a4c0cf5cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt.grad - dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b594a71-8015-46a2-8fd9-500b88e380b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada1047-1a2c-4e5d-99ee-7906dc8ff54b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7da12-d674-4b21-a3d7-19718cc274f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceafdc6c-2f2e-4d1a-a66f-4fce5a0f46be",
   "metadata": {},
   "outputs": [],
   "source": [
    "(rt.grad - dq).flatten(), (kt.grad - dk).flatten(), (vt.grad - dv).flatten(), (ut.grad - du).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff68ae-b9de-4ef7-85c6-d223865ea186",
   "metadata": {},
   "source": [
    "### Retry Inputs without UWMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "id": "9cebc303-1861-46b9-9be5-b5f9fb596920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_inputs(B, H, L, K, V): \n",
    "    th.manual_seed(17)\n",
    "    device = \"cpu\"\n",
    "    rt = th.randn(B, H, L, K, device=device, requires_grad=True)\n",
    "    kt = th.randn(B, H, L, K, device=device, requires_grad=True)\n",
    "    vt = th.randn(B, H, L, V, device=device, requires_grad=True)\n",
    "    wt = -3 + 1e-1 * th.randn(B, H, L, K, device=device)\n",
    "    wt = -th.exp(wt)\n",
    "    wt.requires_grad = True\n",
    "    ut = th.randn(H, K, device=device, requires_grad=True)\n",
    "    return rt, kt, vt, wt, ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9200b4-3824-4a10-84c2-ed3fe90b1717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c011781-3d5b-44d8-845f-c8d8a13f5d64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 19/04/2024 Simplify without U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "76547de8-2e39-47bc-a6ce-efa55e780f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def naive_recurrent(q, k, v, w, initial_state=None, output_final_state=False):\n",
    "    orig_dtype = q.dtype\n",
    "    q, k, v, w = map(lambda x: x.float(), (q, k, v, w))\n",
    "    batch_size, n_heads, seq_len, d_head_k = q.shape\n",
    "    _, _, _, d_head_v = v.shape\n",
    "    h = torch.zeros(batch_size, n_heads, d_head_k, d_head_v, dtype=torch.float32, device=q.device)\n",
    "    o = torch.zeros_like(v)\n",
    "\n",
    "    if initial_state is not None:\n",
    "        h += initial_state\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        q_i = q[:, :, i, :]\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i, :]\n",
    "        w_i = w[:, :, i].exp()\n",
    "        kv_i = k_i[..., None] * v_i[..., None, :]\n",
    "        h = h + w_i[..., None] * kv_i\n",
    "        o_i = h * q_i[..., None]\n",
    "        o[:, :, i] = o_i.sum(-2)\n",
    "        \n",
    "    return o.to(orig_dtype)\n",
    "\n",
    "\n",
    "def get_dw(q, k, v, w, do): \n",
    "    \"\"\" Just test gradient for w \"\"\"\n",
    "    B, H, L, K = q.shape\n",
    "    V = v.shape[-1]\n",
    "    \n",
    "    dw = torch.zeros_like(w)\n",
    "    si = torch.zeros(B, H, K, V)\n",
    "    wsum = w.sum(-2)[..., None] # sum along seqlen\n",
    "    for i in range(L-1, -1, -1): \n",
    "        do_i = do[:, :, i, ..., None, :]\n",
    "        q_i = q[:, :, i, :, None]\n",
    "        k_i = k[:, :, i, :, None] \n",
    "        v_i = v[:, :, i, ..., None, :] \n",
    "        w_i =  w[:, :, i, ..., None]\n",
    "\n",
    "        si += q_i * k_i * v_i * wsum.exp()\n",
    "        wsum -= w_i\n",
    "        dw[:, :, i] = (do_i * si).sum(dim=-2)\n",
    "    return dw\n",
    "\n",
    "def gen_inputs(B, H, L, K, V): \n",
    "    th.manual_seed(17)\n",
    "    device = \"cpu\"\n",
    "    rt = th.randn(B, H, L, K, device=device, requires_grad=True)\n",
    "    kt = th.randn(B, H, L, K, device=device, requires_grad=True)\n",
    "    vt = th.randn(B, H, L, V, device=device, requires_grad=True)\n",
    "    wt = -3 + 1e-1 * th.randn(B, H, L, K, device=device)\n",
    "    wt = -th.exp(wt)\n",
    "    wt.requires_grad = True\n",
    "    ut = th.randn(H, K, device=device, requires_grad=True)\n",
    "    return rt, kt, vt, wt, ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c664719a-e91e-440f-b370-3953e4a48761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-9.3132e-10]), tensor([0.5013]))"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementation is wrong. hidden by high V (ex. 64)\n",
    "B, H, L, K, V = 1, 1, 1, 1, 1\n",
    "\n",
    "#### AUTOGRAD FORWARD PASS\n",
    "rt, kt, vt, wt, ut = gen_inputs(B, H, L, K, V)\n",
    "o = naive_recurrent(rt, kt, vt, wt)\n",
    "grads = []\n",
    "o.register_hook(lambda d:grads.append(d))\n",
    "o.mean().backward()\n",
    "do = grads[0]\n",
    "\n",
    "#### MANUAL BACKWARD PASS\n",
    "rt2, kt2, vt2, wt2, ut2 = gen_inputs(B, H, L, K, V)\n",
    "dw = get_dw(rt2, kt2, vt2, wt2, do)[None]\n",
    "print(dw.shape)\n",
    "(wt.grad - dw).detach().flatten(), (w_ - w2_).detach().flatten()\n",
    "\n",
    "#### DOESN'T MATCH!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "4af73068-3154-415b-9a27-df39a5d9fd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 1])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e41492-d953-4eab-9c1e-8b893541dcd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 19/04/2024 Very simple test (just L=1) - Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "8f643692-6e87-4b9f-a979-35a512141888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.0107]]]]), tensor([[[[1.]]]]))"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementation is wrong. hidden by high V (ex. 64)\n",
    "B, H, L, K, V = 1, 1, 1, 1, 1\n",
    "\n",
    "#### AUTOGRAD FORWARD PASS\n",
    "q, k, v, w, _ = gen_inputs(B, H, L, K, V)\n",
    "\n",
    "o = th.einsum('bhic,bhic,bhic,bhid->bhid', q, w.exp(), k, v)\n",
    "grads = []\n",
    "o.register_hook(lambda d:grads.append(d))\n",
    "o.mean().backward()\n",
    "do = grads[0]\n",
    "w.grad, do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "33dd38c6-9980-483c-8331-429af29e1b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-9.3132e-10]]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad - o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fbeb0-1d07-4c28-bf95-834ceba1a820",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 19/04/2024 Very simple test (just L=2) - ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "900ea23d-76c1-41c4-be5c-dee03b626c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.0227],\n",
       "           [-0.0229]]]]),\n",
       " tensor([[[[0.5000],\n",
       "           [0.5000]]]]))"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementation is wrong. hidden by high V (ex. 64)\n",
    "B, H, L, K, V = 1, 1, 2, 1, 1\n",
    "\n",
    "#### AUTOGRAD FORWARD PASS\n",
    "q, k, v, w, _ = gen_inputs(B, H, L, K, V)\n",
    "\n",
    "s1 = th.einsum('bhc,bhc,bhd->bhcd', w[:, :, 0].exp(), k[:, :, 0], v[:, :, 0])\n",
    "o1 = th.einsum('bhc,bhcd->bhd', q[:, :, 0], s1)\n",
    "s2 = s1 + th.einsum('bhc,bhc,bhd->bhd', w[:, :, :2].sum(dim=2).exp(), k[:, :, 1], v[:, :, 1])\n",
    "o2 = th.einsum('bhc,bhcd->bhd', q[:, :, 1], s2)\n",
    "o = th.stack([o1, o2], dim=2)\n",
    "\n",
    "grads = []\n",
    "o.register_hook(lambda d:grads.append(d))\n",
    "o.mean().backward()\n",
    "do = grads[0]\n",
    "w.grad, do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "54651686-ae93-4dd7-970a-ea3cbf19ad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw2 = th.einsum('bhd,bhcd->bhc', do[:, :, 1], th.einsum('bhc,bhc,bhc,bhd->bhcd', q[:, :, 1], w[:, :, :2].sum(dim=2).exp(), k[:, :, 1], v[:, :, 1]))\n",
    "dw1 = do[:, :, 0] * o[:, :, 0] + do[:, :, 1] * o[:, :, 1]\n",
    "dw = th.stack((dw1, dw2), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "cb0e6ef0-01bb-4c77-8691-f45a6524b9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.8626e-09],\n",
       "          [ 0.0000e+00]]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad - dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ad938-706c-46b9-b657-76ef377975d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 19/04/2024 Very simple test (general L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "c0fd336c-f692-4bce-9afa-d56125e50e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0323,  0.0327,  0.0209,  0.0894,  0.0785,  0.0218,  0.0019, -0.0556,\n",
       "         -0.0575, -0.0579, -0.0265,  0.0239,  0.0093,  0.0209, -0.0112, -0.0085]),\n",
       " tensor([0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156,\n",
       "         0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156,\n",
       "         0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156,\n",
       "         0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156,\n",
       "         0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156,\n",
       "         0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156,\n",
       "         0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156,\n",
       "         0.0156]))"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementation is wrong. hidden by high V (ex. 64)\n",
    "B, H, L, K, V = 1, 1, 16, 1, 4\n",
    "\n",
    "#### AUTOGRAD FORWARD PASS\n",
    "q, k, v, w, _ = gen_inputs(B, H, L, K, V)\n",
    "\n",
    "o = []\n",
    "si = th.zeros(B, H, K, V)\n",
    "for i in range(L): \n",
    "    si = si + th.einsum('bhc,bhc,bhd->bhcd', w[:, :, :i+1].sum(dim=2).exp(), k[:, :, i], v[:, :, i])\n",
    "    oi = th.einsum('bhc,bhcd->bhd', q[:, :, i], si)\n",
    "    o.append(oi)\n",
    "o = th.stack(o, dim=2)\n",
    "\n",
    "grads = []\n",
    "o.register_hook(lambda d:grads.append(d))\n",
    "o.mean().backward()\n",
    "do = grads[0]\n",
    "w.grad.flatten(), do.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "24217b39-c176-43ef-a56b-13f877ada748",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = th.zeros_like(w)\n",
    "doq = th.einsum('bhid,bhic->bhicd', do, q)\n",
    "wkv = th.einsum('bhic,bhic,bhid->bhicd', w.cumsum(dim=2).exp(), k, v)\n",
    "for i in range(L): \n",
    "    doq_wkv = (doq[:, :, i:] * wkv[:, :, [i]]).sum(dim=-1).sum(2)\n",
    "    dw[:, :, :i+1] += doq_wkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "0c53eb22-ac63-475d-88d4-0c8fc56e75cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.2352e-08],\n",
       "          [-2.2352e-08],\n",
       "          [-2.0489e-08],\n",
       "          [-2.2352e-08],\n",
       "          [-2.9802e-08],\n",
       "          [-2.0489e-08],\n",
       "          [-8.3819e-09],\n",
       "          [-3.7253e-09],\n",
       "          [-3.7253e-09],\n",
       "          [ 0.0000e+00],\n",
       "          [ 1.8626e-09],\n",
       "          [-1.8626e-09],\n",
       "          [ 0.0000e+00],\n",
       "          [ 0.0000e+00],\n",
       "          [ 9.3132e-10],\n",
       "          [ 9.3132e-10]]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad - dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc487de-9a14-4ecd-aec8-72659ad6c788",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 19/04/2024 Toy Algo for Backward - purely on W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "4ee5cb1d-b859-432d-9118-734a222da178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation is wrong. hidden by high V (ex. 64)\n",
    "B, H, L, K, V = 1, 1, 16, 1, 4\n",
    "\n",
    "#### AUTOGRAD FORWARD PASS\n",
    "q, k, v, w, _ = gen_inputs(B, H, L, K, V)\n",
    "\n",
    "o = []\n",
    "si = th.zeros(B, H, K, V)\n",
    "for i in range(L): \n",
    "    si = si + th.einsum('bhc,bhc,bhd->bhcd', w[:, :, :i+1].sum(dim=2).exp(), k[:, :, i], v[:, :, i])\n",
    "    oi = th.einsum('bhc,bhcd->bhd', q[:, :, i], si)\n",
    "    o.append(oi)\n",
    "o = th.stack(o, dim=2)\n",
    "o.register_hook(lambda d:grads.append(d))\n",
    "o.mean().backward()\n",
    "do = grads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "400dc0ee-1b04-47d8-9e90-966fb37768c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 7.4506e-09],\n",
       "          [ 7.4506e-09],\n",
       "          [ 1.8626e-09],\n",
       "          [-2.2352e-08],\n",
       "          [ 0.0000e+00],\n",
       "          [-5.5879e-09],\n",
       "          [ 9.3132e-10],\n",
       "          [ 3.7253e-09],\n",
       "          [ 3.7253e-09],\n",
       "          [ 0.0000e+00],\n",
       "          [ 1.8626e-09],\n",
       "          [-5.5879e-09],\n",
       "          [ 0.0000e+00],\n",
       "          [ 0.0000e+00],\n",
       "          [ 9.3132e-10],\n",
       "          [ 9.3132e-10]]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 CUMSUM\n",
    "dw = th.zeros_like(w)\n",
    "doq = th.einsum('bhid,bhic->bhicd', do, q)\n",
    "doq = doq.flip((2,)).cumsum(dim=2).flip((2,))\n",
    "wkv = th.einsum('bhic,bhic,bhid->bhicd', w.cumsum(dim=2).exp(), k, v) # h (guardar durante fw)\n",
    "for i in range(L): \n",
    "    doq_wkv = (doq[:, :, [i]] * wkv[:, :, [i]]).sum(dim=-1).sum(2)\n",
    "    dw[:, :, :i+1] += doq_wkv\n",
    "w.grad - dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "15c02332-9d1c-4221-a7c4-8595dcf21d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0323],\n",
       "          [ 0.0327],\n",
       "          [ 0.0209],\n",
       "          [ 0.0894],\n",
       "          [ 0.0785],\n",
       "          [ 0.0218],\n",
       "          [ 0.0019],\n",
       "          [-0.0556],\n",
       "          [-0.0575],\n",
       "          [-0.0579],\n",
       "          [-0.0265],\n",
       "          [ 0.0239],\n",
       "          [ 0.0093],\n",
       "          [ 0.0209],\n",
       "          [-0.0112],\n",
       "          [-0.0085]]]], grad_fn=<FlipBackward0>)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 cumsums, no loop\n",
    "dw = th.zeros_like(w)\n",
    "doq = th.einsum('bhid,bhic->bhicd', do, q)\n",
    "doq = doq.flip((2,)).cumsum(dim=2).flip((2,))\n",
    "wkv = th.einsum('bhic,bhic,bhid->bhicd', w.cumsum(dim=2).exp(), k, v) # h (guardar durante fw)\n",
    "dw = (doq * wkv).sum(dim=-1)\n",
    "dw = dw.flip((2,)).cumsum(dim=2).flip((2,))\n",
    "dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969f84e-040f-40ca-aac3-c946ddda7092",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 20/04/2024 Toy Algo for Backward - W and U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "20e1ac84-4ec4-4a0c-9006-068b44b551e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation is wrong. hidden by high V (ex. 64)\n",
    "B, H, L, K, V = 1, 1, 16, 1, 4\n",
    "\n",
    "#### AUTOGRAD FORWARD PASS\n",
    "q, k, v, w, _ = gen_inputs(B, H, L, K, V)\n",
    "\n",
    "o = []\n",
    "si = th.zeros(B, H, K, V)\n",
    "for i in range(L): \n",
    "    osi = si + th.einsum('hc,bhc,bhd->bhcd', u, k[:, :, i], v[:, :, i])\n",
    "    oi = th.einsum('bhc,bhcd->bhd', q[:, :, i], osi)\n",
    "    si = si + th.einsum('bhc,bhc,bhd->bhcd', w[:, :, :i+1].sum(dim=2).exp(), k[:, :, i], v[:, :, i])\n",
    "    o.append(oi)\n",
    "o = th.stack(o, dim=2)\n",
    "grads = []\n",
    "o.register_hook(lambda d:grads.append(d))\n",
    "o.mean().backward()\n",
    "do = grads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "32b4204e-349e-4512-9e0d-0d9411338a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO CUMSUMS\n",
    "dw = th.zeros_like(w)\n",
    "doq = th.einsum('bhid,bhic->bhicd', do, q)\n",
    "wkv = th.einsum('bhic,bhic,bhid->bhicd', w.cumsum(dim=2).exp(), k, v)\n",
    "# doq1,wkv0\n",
    "for i in range(L-1): \n",
    "    doq_wkv = (doq[:, :, i+1:] * wkv[:, :, [i]]).sum(dim=-1).sum(2)\n",
    "    dw[:, :, :i+1] += doq_wkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "54e309c2-b623-4906-9879-7f56a6072fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1176e-08, -2.2352e-08, -1.8626e-08, -7.4506e-09, -7.4506e-09,\n",
       "        -3.7253e-09, -9.3132e-10, -3.7253e-09,  0.0000e+00,  3.7253e-09,\n",
       "        -3.7253e-09, -1.8626e-09,  0.0000e+00,  0.0000e+00,  2.3283e-10,\n",
       "         0.0000e+00], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(w.grad - dw).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "0025de52-9a1f-4c85-859d-961f83c804b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.3528e-08, -4.4703e-08, -4.0978e-08, -3.7253e-08, -2.2352e-08,\n",
       "        -1.1176e-08, -9.3132e-09,  3.7253e-09,  7.4506e-09,  3.7253e-09,\n",
       "         3.7253e-09, -1.8626e-09, -9.3132e-10,  0.0000e+00,  2.3283e-10,\n",
       "         0.0000e+00], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 reverse CUMSUM\n",
    "dw = th.zeros_like(w)\n",
    "doq = th.einsum('bhid,bhic->bhicd', do, q)\n",
    "doq = doq.flip((2,)).cumsum(dim=2).flip((2,))\n",
    "wkv = th.einsum('bhic,bhic,bhid->bhicd', w.cumsum(dim=2).exp(), k, v) # h (guardar durante fw)\n",
    "for i in range(L-1): \n",
    "    doq_wkv = (doq[:, :, [i+1]] * wkv[:, :, [i]]).sum(dim=-1).sum(2)\n",
    "    dw[:, :, :i+1] += doq_wkv\n",
    "(w.grad - dw).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "4a9ab54d-ce17-4948-bda1-8d15875a643e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.7253e-08, -4.0978e-08, -4.0978e-08, -2.9802e-08, -2.2352e-08,\n",
       "        -1.3039e-08, -1.0245e-08,  3.7253e-09,  3.7253e-09,  3.7253e-09,\n",
       "         1.8626e-09, -1.8626e-09, -9.3132e-10,  0.0000e+00,  2.3283e-10,\n",
       "         0.0000e+00], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doq = th.einsum('bhid,bhic->bhicd', do, q)\n",
    "doq = doq.flip((2,)).cumsum(dim=2).flip((2,))\n",
    "doq = F.pad(doq, (0,0,0,0,-1,1))\n",
    "wkv = th.einsum('bhic,bhic,bhid->bhicd', w.cumsum(dim=2).exp(), k, v) # h (guardar durante fw)\n",
    "dw = (doq * wkv).sum(dim=-1)\n",
    "dw = dw.flip((2,)).cumsum(dim=2).flip((2,))\n",
    "(w.grad - dw).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "58454411-dada-4e63-b634-d6fdc0249042",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad - dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7c5bdf-6048-4eb9-a457-96671b6f8844",
   "metadata": {},
   "source": [
    "### 20/04/2024 Incorporate algorithm into functions: \n",
    "* [x] Our toy fw pass was wrong: `W` in log space and direct application to the state, etc\n",
    "* [x] Delta with main `FW` function solved\n",
    "* [ ] Need to adjust our backward "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da62efb5-904e-42b0-b15b-c91a57306c12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Base forward func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "b1078dba-2d41-4d7e-939a-f8ecb793891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def simple_inner(q, k, v, w, u): \n",
    "    o = []\n",
    "    si = th.zeros(B, H, K, V)\n",
    "    for i in range(L): \n",
    "        kv = th.einsum('bhc,bhd->bhcd', k[:, :, i], v[:, :, i])\n",
    "        osi = si + th.einsum('hc,bhcd->bhcd', u, kv)\n",
    "        oi = th.einsum('bhc,bhcd->bhd', q[:, :, i], osi)\n",
    "        si = si * w[:, :, i, :, None].exp() + kv\n",
    "        o.append(oi)\n",
    "    o = th.stack(o, dim=2)\n",
    "    return o\n",
    "\n",
    "def songlin_inner(q, k, v, w, u, initial_state=None, output_final_state=False):\n",
    "    orig_dtype = q.dtype\n",
    "    q, k, v, w, u = map(lambda x: x.float(), (q, k, v, w, u))\n",
    "    batch_size, n_heads, seq_len, d_head_k = q.shape\n",
    "    _, _, _, d_head_v = v.shape\n",
    "    h = torch.zeros(batch_size, n_heads, d_head_k, d_head_v, dtype=torch.float32, device=q.device)\n",
    "    o = torch.zeros_like(v)\n",
    "\n",
    "    if initial_state is not None:\n",
    "        h += initial_state\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        q_i = q[:, :, i, :]\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i, :]\n",
    "        w_i = w[:, :, i].exp()\n",
    "        kv_i = k_i[..., None] * v_i[..., None, :]\n",
    "        o_i = (h + u[None, ..., None] * kv_i) * q_i[..., None]\n",
    "        o[:, :, i] = o_i.sum(-2)\n",
    "        h = h * w_i[..., None] + kv_i\n",
    "    return o.to(orig_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09e6e86-db2e-44c0-b658-c95aa97a6bd6",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "id": "6ec7b713-e418-4694-a465-1de8103c7f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00, -1.5418e-04, -5.7530e-03, -5.2927e-03, -4.1082e-02,\n",
       "        -1.6820e-01, -2.4437e-01, -3.0051e-01, -2.6155e-01, -1.6486e-01,\n",
       "        -1.3501e-01, -2.1993e-01, -1.3706e-01, -1.4047e-01, -6.5149e-02,\n",
       "        -3.5055e-02, -3.1937e-02, -2.4309e-02, -6.8515e-02, -6.6513e-02,\n",
       "        -9.0482e-02, -7.5956e-02,  2.1371e-02,  3.1582e-02,  4.1205e-02,\n",
       "        -2.3560e-02, -1.6657e-01,  4.0461e-02,  1.0806e-01,  2.4641e-02,\n",
       "         5.8453e-02,  0.0000e+00])"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementation is wrong. hidden by high V (ex. 64)\n",
    "B, H, L, K, V = 1, 1, 32, 1, 1\n",
    "\n",
    "#### AUTOGRAD FORWARD PASS\n",
    "q, k, v, w, u = gen_inputs(B, H, L, K, V)\n",
    "o = songlin_inner(q, k, v, w, u, th.zeros(B, H, K, V))\n",
    "grads = []\n",
    "o.register_hook(lambda d:grads.append(d))\n",
    "o.mean().backward()\n",
    "do = grads[0]\n",
    "w.grad.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "id": "4050dbfa-891e-4186-a3df-20096ec16a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2, k2, v2, w2, u2 = gen_inputs(B, H, L, K, V)\n",
    "omine = simple_inner(q2, k2, v2, w2, u2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "id": "60aaa188-28c3-4bb6-a0ae-69dbe5857300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00, -1.5418e-04, -5.7530e-03, -5.2927e-03, -4.1082e-02,\n",
       "        -1.6820e-01, -2.4437e-01, -3.0051e-01, -2.6155e-01, -1.6486e-01,\n",
       "        -1.3501e-01, -2.1993e-01, -1.3706e-01, -1.4047e-01, -6.5149e-02,\n",
       "        -3.5055e-02, -3.1937e-02, -2.4309e-02, -6.8515e-02, -6.6513e-02,\n",
       "        -9.0482e-02, -7.5956e-02,  2.1371e-02,  3.1582e-02,  4.1205e-02,\n",
       "        -2.3560e-02, -1.6657e-01,  4.0461e-02,  1.0806e-01,  2.4641e-02,\n",
       "         5.8453e-02,  0.0000e+00])"
      ]
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads2 = []\n",
    "omine.register_hook(lambda d:grads2.append(d))\n",
    "omine.mean().backward()\n",
    "do = grads2[0]\n",
    "w2.grad.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29acf8cd-a594-4553-9c2d-c17d2e431f67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### DW By hand (if L=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "9b3ba36b-fdbf-4663-bee2-8d81d3d8e3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000],\n",
       "          [-0.0045],\n",
       "          [ 0.0000]]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw = th.zeros_like(w)\n",
    "dw[:, :, 1] += do[:, :, 2] * q[:, :, 2] * k[:, :, 0] * v[:, :, 0] * w[:, :, 1].exp()\n",
    "dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ecb7c2-962d-4d20-86a1-d6ad5665b973",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### DW By hand (if L=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "509fc9eb-83d5-44c0-93ba-e2e193bd7ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000, -0.0023, -0.0026,  0.0000], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw = th.zeros_like(w)\n",
    "dw[:, :, 1] += do[:, :, 2] * q[:, :, 2] * k[:, :, 0] * v[:, :, 0] * w[:, :, 1].exp()\n",
    "dw[:, :, 1] += do[:, :, 3] * q[:, :, 3] * k[:, :, 0] * v[:, :, 0] * w[:, :, 1].exp() * w[:, :, 2].exp()\n",
    "dw[:, :, 2] += do[:, :, 3] * q[:, :, 3] * k[:, :, 0] * v[:, :, 0] * w[:, :, 1].exp() * w[:, :, 2].exp()\n",
    "dw[:, :, 2] += do[:, :, 3] * q[:, :, 3] * k[:, :, 1] * v[:, :, 1] * w[:, :, 2].exp()\n",
    "dw.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0803f00-b472-4bb2-b65f-70d10cc4266c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### DW By hand (if L=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "a6b2894f-1f91-4d3e-97a1-702c838a0ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000, -0.0063,  0.1960, -0.0139,  0.0000], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw = th.zeros_like(w)\n",
    "wexp = w.exp()\n",
    "# w2\n",
    "dw[:, :, [1]]       += (do[:, :, 2] * q[:, :, 2] * k[:, :, 0] * v[:, :, 0] * wexp[:, :, 1])[:, :, None]\n",
    "dw[:, :, [1, 2]]    += (do[:, :, 3] * q[:, :, 3] * k[:, :, 0] * v[:, :, 0] * wexp[:, :, 1] * wexp[:, :, 2])[:, :, None]\n",
    "dw[:, :, [1, 2, 3]] += (do[:, :, 4] * q[:, :, 4] * k[:, :, 0] * v[:, :, 0] * wexp[:, :, 1] * wexp[:, :, 2] * wexp[:, :, 3])[:, :, None]\n",
    "# w3\n",
    "dw[:, :, [2]]    += (do[:, :, 3] * q[:, :, 3] * k[:, :, 1] * v[:, :, 1] * wexp[:, :, 2])[:, :, None]\n",
    "dw[:, :, [2, 3]] += (do[:, :, 4] * q[:, :, 4] * k[:, :, 1] * v[:, :, 1] * wexp[:, :, 2] * wexp[:, :, 3])[:, :, None]\n",
    "# dw4 \n",
    "dw[:, :, [3]]    += (do[:, :, 4] * q[:, :, 4] * k[:, :, 2] * v[:, :, 2] * wexp[:, :, 3])[:, :, None]\n",
    "dw.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360185df-ca21-4da8-8ca3-5d11c185b64e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### DW for L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "7141b825-bd89-4bc6-9018-6a604159669c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000, -0.0063,  0.1960, -0.0139,  0.0000], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw = th.zeros_like(w)\n",
    "# el primer w multiplica el estado de 0s y no hace nada. \n",
    "# el ultimo multiplica un estado que no se usa y no hace nada. \n",
    "for i in range(1, L-1): \n",
    "    for j in range(i, L-1): \n",
    "        dw[:, :, i:j+1] += (do[:, :, j+1] * q[:, :, j+1] * k[:, :, i-1] * v[:, :, i-1] * w[:, :, i:j+1].sum(dim=2).exp())[:, :, None]\n",
    "dw.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a906491-413a-4600-9b29-5ee78aa3f06a",
   "metadata": {},
   "source": [
    "#### DW for L: parallel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "id": "4373e659-bb01-4000-80f7-c4392334694b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00, -1.4552e-11,  0.0000e+00,  1.3970e-09,  0.0000e+00,\n",
       "        -1.4901e-08, -2.9802e-08, -2.9802e-08, -8.9407e-08, -1.0431e-07,\n",
       "         0.0000e+00,  1.4901e-08, -4.4703e-08,  4.4703e-08,  7.4506e-09,\n",
       "        -1.4901e-08, -7.4506e-09, -1.6764e-08,  2.9802e-08,  0.0000e+00,\n",
       "        -2.2352e-08, -1.4901e-08,  3.7253e-09,  0.0000e+00,  0.0000e+00,\n",
       "        -1.3039e-08, -4.4703e-08,  3.7253e-09,  2.2352e-08,  3.7253e-09,\n",
       "         3.7253e-09,  0.0000e+00], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw = th.zeros_like(w)\n",
    "# el primer w multiplica el estado de 0s y no hace nada. \n",
    "# el ultimo multiplica un estado que no se usa y no hace nada. \n",
    "doq = th.einsum('bhid,bhic->bhicd', do, q)\n",
    "kv_ = th.einsum('bhic,bhid->bhicd', k, v)\n",
    "for i in range(1, L-1): \n",
    "    for j in range(i, L-1): \n",
    "        wcum = w[:, :, i:j+1].sum(dim=2).exp()\n",
    "        delta = th.einsum('bhcd,bhcd,bhc->bhc', doq[:, :, j+1], kv_[:, :, i-1], wcum )\n",
    "        dw[:, :, i:j+1] += delta[:, :, None]\n",
    "(w2.grad - dw).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61615e1-1836-43ce-84ae-de78041c73a5",
   "metadata": {},
   "source": [
    "##### Move cumsum outside loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "9bab3ebe-d9c6-4d0e-8d71-14012c03c384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00, -1.4508e-04, -5.7476e-03, -5.2867e-03, -4.1076e-02,\n",
       "        -1.6819e-01, -2.4436e-01, -3.0050e-01, -2.6155e-01, -1.6485e-01,\n",
       "        -1.3500e-01, -2.1992e-01, -1.3706e-01, -1.4047e-01, -6.5147e-02,\n",
       "        -3.5055e-02, -3.1936e-02, -2.4308e-02, -6.8514e-02, -6.6512e-02,\n",
       "        -9.0481e-02, -7.5955e-02,  2.1371e-02,  3.1582e-02,  4.1205e-02,\n",
       "        -2.3560e-02, -1.6657e-01,  4.0460e-02,  1.0806e-01,  2.4641e-02,\n",
       "         5.8453e-02,  0.0000e+00], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw = th.zeros_like(w)\n",
    "# el primer w multiplica el estado de 0s y no hace nada. \n",
    "# el ultimo multiplica un estado que no se usa y no hace nada. \n",
    "doq = th.einsum('bhid,bhic->bhicd', do, q)\n",
    "kv_ = th.einsum('bhic,bhid->bhicd', k, v)\n",
    "wcum = w.cumsum(dim=2)\n",
    "for i in range(1, L-1): \n",
    "    for j in range(i, L-1): \n",
    "        delta = th.einsum('bhcd,bhcd,bhc->bhc', doq[:, :, j+1], kv_[:, :, i-1], wcum[:, :, j].exp() )\n",
    "        dw[:, :, i:j+1] += delta[:, :, None]\n",
    "    wcum[:, :, i:] -= wcum[:, :, [i]]\n",
    "dw.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb2e4e-b3f4-4bc2-b50f-7047ce7a74b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 20/04/2024 Check songlin's code for clues - hers is parallel and ours is hard to make fast \n",
    "* [x] Implemented\n",
    "* [x] Check incorrect\n",
    "* [ ] TODO: understand why incorrect\n",
    "* [ ] See if fixable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "74fe33ed-1f53-4509-9488-d69806f6f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qaux(q, k, v, do):\n",
    "    dq_aux = th.zeros(B, H, L, K)\n",
    "    h = th.zeros(B, H, K, V)\n",
    "    for i in range(L):\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i]\n",
    "        w_i = w[:, :, i].exp()\n",
    "        kv_i = k_i[..., None] * v_i[..., None, :]\n",
    "        dq_aux[:, :, i] = (do[:, :, i, None, :] * h).sum(-1)\n",
    "        h = h * w_i[..., None] + kv_i\n",
    "    return dq_aux\n",
    "\n",
    "def get_kaux(q, k, v, do): \n",
    "    dk_aux = th.zeros(B, H, L, K)\n",
    "    dh = th.zeros(B, H, K, V)\n",
    "    for i in range(L-1, -1, -1):\n",
    "        d_kv_i = do[:, :, i, None, :] * q[:, :, i, :, None]\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i]\n",
    "        dk_aux[:, :, i] = (dh * v_i[..., None, :]).sum(-1)\n",
    "        dh = dh * w[:, :, i, :, None].exp() + d_kv_i\n",
    "    return dk_aux\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "47c28bb3-023a-4924-b9e7-d7ed625d42a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.9849e-10, -2.2847e-03, -2.5597e-03,  0.0000e+00],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw = th.zeros_like(w)\n",
    "dk_aux = get_kaux(q, k, v, do)\n",
    "dq_aux = get_qaux(q, k, v, do)\n",
    "for i in range(L-2, -1, -1): \n",
    "    dw[:, :, i] = dw[:, :, i+1] + dq_aux[:, :, i+1] * q[:, :, i+1] - dk_aux[:, :, i] * k[:, :, i]\n",
    "dw.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761cf096-88d0-460a-bf59-568aa7246946",
   "metadata": {},
   "source": [
    "### 20/04/2024 Go over backward pass again\n",
    "* [x] Expand as sum of parallelizable terms\n",
    "* [x] Get recurrence relation\n",
    "* [x] Assimilate to Songlin's notation: **But can't be! Her code is numerically wrong with just L=4 (ERR `~1e-4`) and our code was always correct: `Err ~ 1-8 on l=32`**\n",
    "* [ ] Rewrite songlin's code to to match our notation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "00cfe465-dc74-4f02-a232-b909e6d6473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qaux_mine(q, k, v, do):\n",
    "    dq_aux = th.zeros(B, H, L, K)\n",
    "    h = th.zeros(B, H, K, V)\n",
    "    for i in range(L):\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i]\n",
    "        w_i = w[:, :, i].exp()\n",
    "        kv_i = k_i[..., None] * v_i[..., None, :]\n",
    "        dq_aux[:, :, i] = (do[:, :, i, None, :] * h).sum(-1)\n",
    "        h = h * w_i[..., None] + kv_i\n",
    "        \n",
    "    return dq_aux\n",
    "\n",
    "def get_kaux_mine(q, k, v, do): \n",
    "    dk_aux = th.zeros(B, H, L, K)\n",
    "    dh = th.zeros(B, H, K, V)\n",
    "    hstr = \"\"\n",
    "    for i in range(L-1, -1, -1):\n",
    "        d_kv_i = do[:, :, i, None, :] * q[:, :, i, :, None]\n",
    "        v_i = v[:, :, i]\n",
    "        dk_aux[:, :, i] = (dh * v_i[..., None, :]).sum(-1)\n",
    "        # print(f\"dkaux_{i}= ({hstr}) * v_{i}\")\n",
    "        dh = dh * w[:, :, i, :, None].exp() + d_kv_i\n",
    "        # hstr = f\"({hstr}) * w_{i} + do_{i} * o_{i}\"\n",
    "        # print(hstr, dh)\n",
    "    return dk_aux\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0673da44-5937-4ad9-8022-f8f094e93c2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Dev version\n",
    "* WARNING! Skip the 0 update for weird numerical precision errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "id": "66e1d693-d73b-494c-a95f-b3496869bca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.2048e-07, 1.2014e-07, 1.2107e-07, 1.2293e-07, 1.1921e-07,\n",
       "        1.0431e-07, 8.9407e-08, 5.9605e-08, 4.4703e-08, 2.9802e-08, 2.9802e-08,\n",
       "        1.4901e-08, 1.4901e-08, 1.4901e-08, 1.8626e-08, 1.8626e-08, 1.8626e-08,\n",
       "        2.2352e-08, 2.2352e-08, 2.2352e-08, 1.4901e-08, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 3.7253e-09, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8626e-09,\n",
       "        0.0000e+00, 0.0000e+00], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 857,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dk_aux = get_kaux_mine(q, k, v, do)\n",
    "dq_aux = get_qaux_mine(q, k, v, do)\n",
    "dw = th.zeros_like(w)\n",
    "# WARNING! Skip the 0 update for weird numerical precision errors\n",
    "for i in range(L-2, 0, -1): \n",
    "    dw[:, :, i] = dw[:, :, i+1] + dq_aux[:, :, i+1] * q[:, :, i+1] - dk_aux[:, :, i] * k[:, :, i]\n",
    "(w2.grad - dw).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "id": "a2fbda97-3293-4ea3-8a54-3e9c866585f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00],\n",
       "          [-2.3283e-09],\n",
       "          [ 0.0000e+00],\n",
       "          [ 0.0000e+00],\n",
       "          [ 0.0000e+00]]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 849,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad - dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "id": "0378b1d4-f3b9-4d88-9ca6-584ccc9678a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dw2 = dq_aux[:, :, 3] * q[:, :, 3] - dk_aux[:, :, 2] * k[:, :, 2]\n",
    "# dw1 = dw2 + dq_aux[:, :, 2] * q[:, :, 2] - dk_aux[:, :, 1] * k[:, :, 1]\n",
    "# dw2, dw1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0822d4e-a182-4108-adba-6a131c6fe631",
   "metadata": {},
   "source": [
    "#### Reduce numerical errors! - Use cumsums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "4617a80f-a294-4db1-8747-337e1420c302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00,  6.8729e-08,  6.8452e-08,  6.8918e-08,  7.0781e-08,\n",
       "         5.9605e-08,  5.9605e-08,  5.9605e-08,  2.9802e-08,  2.9802e-08,\n",
       "         2.9802e-08,  2.9802e-08,  1.4901e-08,  1.4901e-08,  1.4901e-08,\n",
       "         1.1176e-08,  1.1176e-08,  9.3132e-09,  1.4901e-08,  7.4506e-09,\n",
       "         0.0000e+00,  0.0000e+00, -1.3039e-08, -1.1176e-08, -1.1176e-08,\n",
       "        -5.5879e-09,  0.0000e+00, -3.7253e-09,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 880,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dk_aux = get_kaux_mine(q, k, v, do)\n",
    "dq_aux = get_qaux_mine(q, k, v, do)\n",
    "dw = th.zeros_like(w)\n",
    "# WARNING! Skip the 0 update for weird numerical precision errors\n",
    "delta = (dq_aux[:, :, 1:] * q[:, :, 1:] - dk_aux[:, :, :-1] * k[:, :, :-1]).flip((2,)).cumsum(dim=2).flip((2,))\n",
    "dw = F.pad(delta, (0, 0, 0, 1))\n",
    "dw[:, :, 0] = 0.\n",
    "(w2.grad - dw).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8443a319-6988-4306-a4c7-0eb392bdd254",
   "metadata": {},
   "source": [
    "### 20/04/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2d4e32-f874-46bf-802c-50d071ec44a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "####  Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "id": "a7f2b9cf-a0d7-48c2-af1b-9cd4fc6a2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def songlin_bw(q, k, v, w, u, o, do, initial_state=None, output_final_state=False):\n",
    "    q, k, v, w, u, o, do = map(lambda x: x.float(), (q, k, v, w, u, o, do))\n",
    "    batch_size, n_heads, seq_len, d_head_k = q.shape\n",
    "    _, _, _, d_head_v = v.shape\n",
    "    h = torch.zeros(batch_size, n_heads, d_head_k, d_head_v, dtype=torch.float32, device=q.device)\n",
    "    dq = torch.zeros_like(q)\n",
    "    dq_aux = torch.zeros_like(q)\n",
    "\n",
    "    if initial_state is not None:\n",
    "        h += initial_state\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i]\n",
    "        w_i = w[:, :, i].exp()\n",
    "        kv_i = k_i[..., None] * v_i[..., None, :]\n",
    "        h_i = (h + u[None, ..., None] * kv_i)\n",
    "        dq_i = (do[:, :, i, None, :] * h_i).sum(-1)\n",
    "        dq_aux_i = (do[:, :, i, None, :] * h).sum(-1)\n",
    "        dq[:, :, i] = dq_i\n",
    "        dq_aux[:, :, i] = dq_aux_i\n",
    "        h = h * w_i[..., None] + kv_i\n",
    "\n",
    "    du = u.new_zeros(batch_size, n_heads, d_head_k)\n",
    "    dh = torch.zeros_like(h)\n",
    "    dk = torch.zeros_like(k)\n",
    "    dk_aux = torch.zeros_like(k)\n",
    "    dv = torch.zeros_like(v)\n",
    "\n",
    "    for i in range(seq_len-1, -1, -1):\n",
    "        d_kv_i = do[:, :, i, None, :] * q[:, :, i, :, None]\n",
    "        k_i = k[:, :, i]\n",
    "        v_i = v[:, :, i]\n",
    "        du_i = (d_kv_i * k_i[..., None] * v_i[..., None, :]).sum(-1)\n",
    "        du += du_i\n",
    "        dk_i = (dh * v_i[..., None, :]).sum(-1)\n",
    "        dk_aux[:, :, i] = dk_i\n",
    "        dk_i += (d_kv_i * u[None, ..., None] * v_i[..., None, :]).sum(-1)\n",
    "        dv_i = (d_kv_i * u[None, ..., None] * k_i[..., None]).sum(-2)\n",
    "        dv_i += (dh * k_i[..., None]).sum(-2)\n",
    "\n",
    "        dk[:, :, i] = dk_i\n",
    "        dv[:, :, i] = dv_i\n",
    "        dh = dh * w[:, :, i, :, None].exp() + d_kv_i\n",
    "\n",
    "    # dw = q * dq_aux - k * dk_aux\n",
    "    # dw = torch.zeros_like(w)\n",
    "    # for i in range(seq_len-2, 0, -1):\n",
    "    #     dw[:, :, i] = dw[:, :, i+1] + dq_aux[:, :, i+1] * q[:, :, i+1] - dk_aux[:, :, i] * k[:, :, i]\n",
    "    # delta = reverse cumsum of q*q_aux - k*k_aux\n",
    "    delta = (dq_aux[:, :, 1:] * q[:, :, 1:] - dk_aux[:, :, :-1] * k[:, :, :-1]).flip((2,)).cumsum(dim=2).flip((2,))\n",
    "    dw = F.pad(delta, (0, 0, 0, 1))\n",
    "\n",
    "    du = du.sum(0)\n",
    "    return dq, dk, dv, dw, du"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fab203-95e8-42fc-a18c-bcfb15e2f8c2",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "id": "363fb86d-6650-40c5-ab3f-82b389b18fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddq, ddk, ddv, ddw, ddu = songlin_bw(q, k, v, w, u, o, do, initial_state=None, output_final_state=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "id": "7d86b89a-1103-448d-b02e-23ce720507e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.8743e-08,  6.8729e-08,  6.8452e-08,  6.8918e-08,  7.0781e-08,\n",
       "         5.9605e-08,  5.9605e-08,  5.9605e-08,  2.9802e-08,  2.9802e-08,\n",
       "         2.9802e-08,  2.9802e-08,  1.4901e-08,  1.4901e-08,  1.4901e-08,\n",
       "         1.1176e-08,  1.1176e-08,  9.3132e-09,  1.4901e-08,  7.4506e-09,\n",
       "         0.0000e+00,  0.0000e+00, -1.3039e-08, -1.1176e-08, -1.1176e-08,\n",
       "        -5.5879e-09,  0.0000e+00, -3.7253e-09,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(w2.grad - ddw).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3b074-d19e-4bef-9517-b06386e2dec1",
   "metadata": {},
   "source": [
    "### 20/04/2024 Error was a simple padding by 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff9fc5-054a-4217-8f4c-fe70cc81198b",
   "metadata": {},
   "source": [
    "## AOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a389a8-7d8c-4daf-b38a-cb4f03bcdce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f628781-e19f-48b2-89d0-492cae71a4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46be9d-c48b-468f-893e-47f1b0bf0961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce236ac2-06fb-4c1e-9a58-be1ad89f8361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f8af5-8c64-430b-bba3-4811166048f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845acb17-e551-4533-b5e6-9d8abe78c63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d01a4f-e8c2-4ace-b52e-6c956d601cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f86fb-984b-433a-88a2-cf672e6f68b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32354ee-bf52-479a-ac8b-e97c245ff9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa51c30-bc1b-48fa-8a17-2e8f5cd3dd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "653ed5cb-4ce9-4edd-af6d-e8e9b6f9f3f4",
   "metadata": {},
   "source": [
    "### RWKV6Plus test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac610e32-6322-4f9b-a585-f16c0662b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "def rwkv_inner_v6plus(\n",
    "    r,\n",
    "    k,\n",
    "    v,\n",
    "    w,\n",
    "    u,\n",
    "    kv_state,\n",
    "    chunk_len: int = 24,\n",
    "    precision_dtype: torch.dtype = torch.float32,\n",
    "    precision_min_val: float = 0.005,\n",
    "):\n",
    "    \"\"\"\n",
    "    expects\n",
    "    r : (B,H,L,K)\n",
    "    k : (B,H,L,K)\n",
    "    v : (B,H,L,V)\n",
    "    w : (B,H,L,K) or (1,H,L,K)\n",
    "    u : (1,H,1,K,V)\n",
    "    kv_state : (B,H,K,V)\n",
    "    \"\"\"\n",
    "    B, H, L, K = k.size()\n",
    "    V = v.size(-1)\n",
    "    T = chunk_len\n",
    "\n",
    "    if L == 1:\n",
    "        kv = k.mT @ v\n",
    "        out = r @ (kv_state + u.mT * kv)\n",
    "        kv_state = w.mT * kv_state + kv\n",
    "        return out, kv_state\n",
    "    else:\n",
    "        # FIXME - support fast path for non-exact multiples\n",
    "        # ensure it's an exact multiple\n",
    "        if L % T != 0:\n",
    "            T = 1\n",
    "\n",
    "        N = L // T\n",
    "\n",
    "        w = w.clamp(precision_min_val)\n",
    "\n",
    "        # calculate cumulative decay in log space where it won't overflow\n",
    "        w_log = w.float().log()  # (1,H,L,K) or (B,H,L,K)\n",
    "\n",
    "        # chunked view of w_log\n",
    "        wc_log = w_log.view(w.size(0), H, N, T, K)\n",
    "        wc_log_cum = wc_log.cumsum(dim=-2)\n",
    "\n",
    "        # chunked view of shifted_w_log\n",
    "        shifted_wc_log_cum = F.pad(wc_log_cum, (0, 0, 1, -1))\n",
    "\n",
    "        # NOTE - we have to apply the decay weight from TWO ahead.. ONE ahead gets no decay (log==0)\n",
    "        # pre-applied weights\n",
    "        # left side is prior chunk (w_inter), right side is current chunk (w_intra)\n",
    "        # without u...\n",
    "        # w0   w1   w2   w3   | w4   w5   w6   w7\n",
    "        # w1:4 w2:4 w3:4 w4:4 | w4:5 w4:6 w4:7 w4:8\n",
    "        # with u...\n",
    "        # w0   w1   w2   w3   | w4   w5   w6   w7\n",
    "        # w1:4 w2:4 w3:4 w4:4 | w4:4 w4:5 w4:6 w4:7\n",
    "\n",
    "        # ws decays the entire current state (representing t-1) to the prior block (t-2)\n",
    "        ws = wc_log.sum(dim=-2, keepdim=True)  # 1HN1K or BHN1K\n",
    "        # w_inter is the decay to the end of the current block, since it will be applied at the next iteration when current (t) becomes prior (t-1)\n",
    "        # this formula because e.g. w1:4 = w0:4 - w0:1\n",
    "        w_inter = ws - wc_log_cum  # 1HNTK or BHNTK (w^(T-1) ... w^0)\n",
    "        # w_intra is the decay from the beginning of the current block (t), since it will be applied to current queries (t) against prior state (representing keys+values up to but not including block t)\n",
    "        # this formula because e.g. w1:3 = w0:3 - w0\n",
    "        w_intra = wc_log_cum - wc_log  # 1HNTK or BHNTK (w^0 ... w^(T-2))\n",
    "\n",
    "        ws = list(ws.mT.exp().to(r.dtype).unbind(dim=-3))  # N x 1HK1 or BHK1 !!NOTE THE .mT HERE!!\n",
    "        w_inter = w_inter.exp().to(r.dtype)  # 1HNTK or BHNTK\n",
    "        w_intra = w_intra.exp().to(r.dtype)  # 1HNTK or BHNTK\n",
    "\n",
    "        # precompute u contrib\n",
    "        u = u.squeeze(0).squeeze(-3).to(r.dtype)\n",
    "        uterm_out = torch.einsum('bhic,hcd,bhic,bhid->bhid', r, u, k, v)\n",
    "\n",
    "        # chunked view of r, k, v\n",
    "        r = r.view(B, H, N, T, K)\n",
    "        k = k.view(B, H, N, T, K)\n",
    "        v = v.view(B, H, N, T, V)\n",
    "\n",
    "        # parallel calculation of all intra-chunk attention contributions\n",
    "        wc_log_offset = shifted_wc_log_cum[..., T // 2 : T // 2 + 1, :]  # B,H,N,1,K\n",
    "        r_decay = (shifted_wc_log_cum - wc_log_offset).to(precision_dtype).exp()  # B,H,N,T,K\n",
    "        k_inv_decay = (wc_log_offset - wc_log_cum).to(precision_dtype).exp()  # B,H,N,T,K\n",
    "        a = ((r * r_decay) @ (k * k_inv_decay).mT).to(r.dtype).tril(-1)  # B,H,N,T,T\n",
    "        # add u term to attention (NOTE - the tril(-1) above zeroed the diagonal)\n",
    "        # a = a + torch.einsum(\"bhntk,bhntk->bhnt\", r, u * k).diag_embed()\n",
    "        out = a @ v  # BHNTV\n",
    "        # alternate way of adding in u\n",
    "        # out = out + torch.einsum('bhntk,bhntk,bhntv->bhntv', r, u * k, v)\n",
    "\n",
    "        # parallel precalculation of chunked (k*wk).mT@v for use in recurrent state calc below\n",
    "        wkv = (k * w_inter).mT @ v  # BHNKV\n",
    "        wkv = list(wkv.unbind(dim=-3))  # N x BHKV\n",
    "\n",
    "        # recurrent calculation of all states\n",
    "        states = []\n",
    "        for i in range(N):\n",
    "            states.append(kv_state)\n",
    "            kv_state = kv_state * ws[i] + wkv[i]  # BHKV\n",
    "            # equivalent non-precalced version\n",
    "            # wkv = (k[...,i,:,:] * wk[...,i,:,:]).mT @ v[...,i,:,:]\n",
    "            # kv_state = kv_state * ws[i] + wkv\n",
    "        states = torch.stack(states, dim=2)  # BHNKV\n",
    "\n",
    "        # parallel application of all r to states\n",
    "        out = out + (r * w_intra) @ states  # BHNTV\n",
    "        out = out.view(B, H, L, V)\n",
    "\n",
    "        out = out + uterm_out\n",
    "\n",
    "        return out, kv_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e1ada7c-e8c3-4574-8b61-710697c6797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "def gen_inputs(B, H, L, K, V): \n",
    "    th.manual_seed(17)\n",
    "    device = \"cpu\"\n",
    "    rt = th.randn(B, H, L, K, device=device, requires_grad=True)\n",
    "    kt = th.randn(B, H, L, K, device=device, requires_grad=True)\n",
    "    vt = th.randn(B, H, L, V, device=device, requires_grad=True)\n",
    "    wt = -2 + 1e-1 * th.randn(B, H, L, K, device=device)\n",
    "    wt = -th.exp(wt)\n",
    "    wt.requires_grad = True\n",
    "    ut = th.randn(H, K, V, device=device, requires_grad=True)\n",
    "    return rt, kt, vt, wt, ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "789804f0-4faf-4a81-ab43-fe597332e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, H, L, K, V = 1, 2, 8, 4, 4\n",
    "rt, kt, vt, wt, ut = gen_inputs(B, H, L, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a5c9918-5f4e-4403-bad0-5513aa5607de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.0311,  0.1554, -0.0575, -0.1376],\n",
       "           [ 1.1602,  0.1685,  0.0585, -0.2987],\n",
       "           [ 0.6814, -0.1242, -0.8078, -0.6277],\n",
       "           [ 2.6293, -1.1666,  0.0436,  0.4222],\n",
       "           [-1.5961, -0.9543,  0.3645,  1.4839],\n",
       "           [-0.3761, -0.6710,  0.2651,  0.1544],\n",
       "           [ 1.2655,  1.7080,  2.6342,  2.4107],\n",
       "           [-1.0588,  0.7008,  0.8531,  0.7806]],\n",
       " \n",
       "          [[ 0.0457,  0.1455, -0.3365, -0.5071],\n",
       "           [ 0.1446, -2.2181, -1.6158,  2.7262],\n",
       "           [-1.0139,  2.1165,  6.7032, -0.8199],\n",
       "           [-0.9424,  2.4368,  2.2276,  0.7987],\n",
       "           [ 0.6797,  1.7517,  1.6228,  0.1750],\n",
       "           [-0.2344,  0.1294, -1.1555,  0.5780],\n",
       "           [-2.9541, -0.0378,  9.9134, -3.1395],\n",
       "           [-1.6844, -0.3199,  3.7610,  1.7704]]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[[-1.8724e-01, -2.7551e+00,  2.1327e+00, -4.3973e-02],\n",
       "           [ 3.5296e-02,  3.3468e-01, -2.7179e-01, -2.5437e-03],\n",
       "           [-1.2411e-01, -1.7561e+00,  1.3643e+00, -2.5014e-02],\n",
       "           [-5.4613e-02, -8.3184e-01,  6.4203e-01, -1.4433e-02]],\n",
       " \n",
       "          [[ 1.7435e-02,  1.5875e-01,  1.1704e-01,  4.7224e-01],\n",
       "           [ 2.3769e-02,  2.4388e-01,  1.8584e-01,  7.2728e-01],\n",
       "           [-1.9469e-02, -2.2329e-01, -1.7485e-01, -6.6722e-01],\n",
       "           [-9.4888e-03, -3.2510e-02, -1.1779e-02, -9.3271e-02]]]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwkv_inner_v6plus(rt, kt, vt, wt, ut, torch.zeros(B,H,K,V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e09d423-0c67-4af7-ab91-660fd6a05c40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
